{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: **Invariance** and **Equivariance** at different layers of a *CNN*\n",
    "\n",
    "Advanced Topics in Machine Learning -- Fall 2024, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/notebooks/AdvML_UniTS_2024_Lab_05_CNN_Invariance_Equivariance.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of the *Lab*\n",
    "\n",
    "In the following *Lab*, we will study the *invariance* and *equivariance* properties of specific layers within a *CNN*.\n",
    "\n",
    "Recall the definitions -- respectively -- of **invariance** and **equivariance** of (the result of) function $f$ with respect to transformation (expressed in the form of an operator) $P_{\\alpha}$ parametrized by $\\alpha$:\n",
    "\n",
    "- *Invariance*: $f(P_{\\alpha} x) = f(x)\\;\\;\\;\\; \\forall\\alpha$\n",
    "- *Equivariance*: $f(P_{\\alpha} x) = P_{\\alpha} f(x)\\;\\;\\;\\; \\forall\\alpha$\n",
    "\n",
    "According to theory, the training of a *CNN* with pooling should lead to a network whose:\n",
    "\n",
    "- *Convolutional* layers are *equivariant* to translation;\n",
    "- *Fully Connected* layers are *invariant* to translation.\n",
    "\n",
    "Due to the specific structure of convolutional layers, it is possible to show that the *equivariance* property gives rise to permuted activations in response to translation of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following *CNN* model is given, whose output -- for your convenience -- is a tuple, composed of the actual output of the network, the activation tensor after the second *convolutional* layer, and the activation tensor after the first *fully-connected* layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(729, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        conv2repr = x.clone().detach()\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        fc1repr = x.clone().detach()\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x, conv2repr, fc1repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking inspiration from previous *Labs*:\n",
    "\n",
    "1. Train the model on the (non-augmented) *MNIST* dataset;\n",
    "2. Prepare a test dataset composed of pairs of mutually traslated images;\n",
    "3. Extract the activations of layers `conv2` and `fc1` and check whether they respect the invariance/equivariance property.\n",
    "\n",
    "**Hint**: To test for *equivariance*, it may be useful to notice that **sorting** is invariant to permutations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkibvpTEMRILBn2/x8IuJj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
