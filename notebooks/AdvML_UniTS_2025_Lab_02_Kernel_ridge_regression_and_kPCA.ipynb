{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "authorship_tag": "ABX9TyPkibvpTEMRILBn2/x8IuJj"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Lab 2: *Kernelized* machine learning\n",
                "\n",
                "Advanced Topics in Machine Learning -- Fall 2025, UniTS\n",
                "\n",
                "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/notebooks/AdvML_UniTS_2025_Lab_02_Kernel_ridge_regression_and_kPCA.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### *Kernel Regression* and *Kernel Ridge Regression*\n",
                "\n",
                "Recall that the solution of **linear regression** can also be written as: $w=(X^{T}X)^{-1}X^{T}y=X^{T}(XX^{T})^{-1}y$\n",
                "\n",
                "Let $X\\in R^{N\\times d}$: we have $X^{T}X\\in R^{d\\times d}$ and $K=XX^{T}\\in R^{N\\times N}$. Whether it is more convenient to (pre)compute which matrix product (among $X^{T}X$, $XX^{T}$) depends on the $d/N$ ratio.\n",
                "\n",
                "As far as predictions are concerned, we have that: $f(z)=z^{T}w=z^{T} X^{T}(XX^{T})^{-1}y= \\alpha^{T}(z)K^{-1}y$, with $\\alpha(z)=z^{T}X^{T}=K(z,X)\\in R^{1\\times N}$.\n",
                "\n",
                "How can we move to the non-linear regression case?  We just substitute $x\\rightarrow \\phi(x)$, and the reasoning above can be repeated!\n",
                "\n",
                "For more information, you can look up [this Medium article](https://knork.medium.com/linear-regression-in-python-from-scratch-with-kernels-e9c37f7975b9) or [this code-first tutorial](https://github.com/luigicarratino/Tutorial_Kernels_MLSS2019_London/blob/master/Tutorial%20Kernel.ipynb)."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "from sklearn.datasets import make_moons"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Exercise 1: linear data regression**\n",
                "1. Generate and plot a dataset consisting in $100$ datapoints in the form $(x_i,y_i)$. The $x_i$ are sampled uniformly in $[2,30]$, whereas the $y_i$ are sampled from a Gaussian distribution centred in $2x_i + 50$ having unit variance."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "outputs": [],
            "source": [
                "# YOUR CODE HERE"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "2. Fit a linear regression model to the data, **with no learnable intercept** (i.e. fix it to zero), and plot the predictions of the resulting model compared to the data."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "outputs": [],
            "source": [
                "# YOUR CODE HERE"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "3. Fit a linear regression model to the data, **with learnable intercept**, and plot the predictions of the resulting model compared to the data."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "outputs": [],
            "source": [
                "# YOUR CODE HERE"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "4. Fit a linear ridge regression model to the data, and plot the predictions of the resulting model compared to the data."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "outputs": [],
            "source": [
                "# YOUR CODE HERE"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Exercise 2: Kernel regression on *periodic* data**\n",
                "1. The following dataset is given. Plot it."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "outputs": [],
            "source": [
                "xkr = np.linspace(2, 30, 100)\n",
                "ykr = xkr + 4 * np.sin(xkr) + 4 * np.random.rand(xkr.shape[0])"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "outputs": [],
            "source": [
                "# YOUR CODE HERE"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "2. Define a function that computes the Gaussian kernel value between two vectors, represented as numpy arrays."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "outputs": [],
            "source": [
                "def gaussian_kernel(x1, x2, sigma):\n",
                "    # YOUR CODE HERE\n",
                "    ..."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "3. With the fuction just defined, compute the Gram matrix of the dataset. Use a Gaussian kernel with $\\sigma=1$."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "outputs": [],
            "source": [
                "# YOUR CODE HERE"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "4. Fit a kernel ridge regression model to the data, and plot the resulting model compared to the data."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "outputs": [],
            "source": [
                "# YOUR CODE HERE"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### *Kernel PCA* and the *Radial Basis Function* (RBF) Kernel\n",
                "\n",
                "Let us briefly recap the key concepts about PCA. We have a dataset $X\\in R^{N\\times d}$, and we want to find a new basis $Z\\in R^{N\\times d}$ such that the explained variance of the data, projected on the new basis, is maximized for any fixed number of components. This is equivalent to finding the eigenvectors of the covariance matrix $C=X^{T}X$.\n",
                "\n",
                "Such operation requires the data to be centered, i.e. $X_{i}^{(j)}-\\mu_{j} \\rightarrow X_{i}^{(j)}$, where $\\mu_{j}=\\frac{1}{N}\\sum_{i=1}^{N}X_{i}^{(j)}$ is the mean of the $j$-th feature.\n",
                "\n",
                "We can also write the eigenvalue problem component-wise, as follows: $Cz_{j}=\\lambda_{j}z_{j}$, where $z_{j}$ is the $j$-th eigenvector and $\\lambda_{j}$ is the corresponding eigenvalue. The eigenvectors are orthogonal, i.e. $z_{j}^{T}z_{k}=0$ for $j\\neq k$.\n",
                "\n",
                "Moving to the kernelized case, we operate the substitution $x\\rightarrow \\phi(x)$, and the eigenvalue problem becomes $C_{K}v=\\lambda v$ with $C_{K}=\\frac{1}{N}\\sum_{i}\\phi(x_{i})\\phi^{T}(x_{i})$, with $\\phi(\\cdot)$ being a generic feature map.\n",
                "\n",
                "One can prove that solutions of the eigenvalue problem are in the form $v=\\sum_{i}\\alpha_{i}\\phi(x_{i})$. Multiplying both sides of $C_{K}v=\\lambda v$ by $\\phi(x_{k})$ and substituting, we obtain $N\\lambda \\alpha=K \\alpha$.\n",
                "\n",
                "Such reasoning still requires normalization, i.e. $ \\phi(x)-\\frac{1}{d}\\sum_{i}(\\phi(x))_{i} \\rightarrow \\phi(x)$. The resulting kernel $K$ is in the form $K := K-2Id_{1/n}K + Id_{1/n}K Id_{1/n}$ with $Id_{1/n}$ the matrix with entries $1/n$.\n",
                "\n",
                "A more thorough explanation can be found in [this blogpost](https://sdat.ir/en/sdat-blog/python-kernel-tricks-and-nonlinear-dimensionality-reduction-via-rbf-kernel-pca).\n"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Exercise 3: Kernel PCA**\n",
                "1. A *Half Moons* dataset of $100$ points is given below. Plot it."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "outputs": [],
            "source": [
                "xhm, yhm = make_moons(n_samples=100, random_state=123)"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "outputs": [],
            "source": [
                "# YOUR CODE HERE"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "2. Apply the PCA algorithm to the dataset, and plot the resulting projection using the first 2 principal components.\n",
                "**Hint**: you can use the `PCA` class from `sklearn.decomposition`."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "outputs": [],
            "source": [
                "# YOUR CODE HERE"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "3. Repeat the previous step, but plotting just the first principal component. Comment on the separability of the two classes."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "outputs": [],
            "source": [
                "# YOUR CODE HERE"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "4. Implement a function that computes the *Radial Basis* PCA algorithm of the dataset, given as a NumPy array."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "outputs": [],
            "source": [
                "def kpca(x_data, gamma, n_components):\n",
                "    \"\"\"\n",
                "    Implementation of a RBF kernel PCA.\n",
                "\n",
                "    Arguments:\n",
                "        x_data: A MxN dataset as NumPy array where the samples are stored as rows (M),\n",
                "           and the attributes defined as columns (N).\n",
                "        gamma: The free parameter (coefficient) for the RBF kernel.\n",
                "        n_components: The number of components to be returned.\n",
                "\n",
                "    \"\"\"\n",
                "    # YOUR CODE HERE\n",
                "    xpcs = ...\n",
                "    return xpcs"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "5. Apply the function just defined to the dataset, and plot the resulting projection using the first 2 principal components."
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "outputs": [],
            "source": [
                "# YOUR CODE HERE"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "6. Repeat the previous step, but plotting just the first principal component. Comment on the separability of the two classes.\n"
            ],
            "metadata": {
                "collapsed": false
            }
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "outputs": [],
            "source": [
                "# YOUR CODE HERE"
            ],
            "metadata": {
                "collapsed": false
            }
        }
    ]
}
