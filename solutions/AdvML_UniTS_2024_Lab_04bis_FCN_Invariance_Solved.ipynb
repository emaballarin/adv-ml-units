{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Solved] Lab 4 bis: **Invariance** in a *shallow FCN* under data augmentation\n",
    "\n",
    "Advanced Topics in Machine Learning -- Fall 2023, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/solutions/AdvML_UniTS_2023_Lab_04bis_FCN_Invariance_Solved.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-level overview\n",
    "\n",
    "In this *Lab*, we will understand how the effect of *data augmentation* (whose effect on learned **weights** has been analyzed in the previous lab) translates to the **representation** learned by the model.\n",
    "\n",
    "Specifically, we define *representation* the (ordered) set of activations of a *neural network* model, which is dependent on the input, and can be seen as the way the model *sees* the data as a result of the learning process.\n",
    "\n",
    "To accomplish this goal, we will:\n",
    "\n",
    "- Load the weights resulting from the training of the model described in the previous lab;\n",
    "- Learn how to extract the activations of a given layer of the model, in response to a given input;\n",
    "- Evaluate such activations on mutually-rotated versions of the same input, and compare such activations to assess their *invariance*  with respect to the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary: adapt and re-run the previous notebook\n",
    "\n",
    "Before starting to delve into this lab, you should:\n",
    "- Go back to the previous *Lab* notebook;\n",
    "- Add the (single line of) code required to save the model weights after training;\n",
    "- Re-run the notebook, to make sure that the weights are saved correctly;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T00:59:23.883658900Z",
     "start_time": "2023-09-17T00:59:22.181232Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Re-)definition of the model\n",
    "\n",
    "Define the exact same model you used in the previous lab, and instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T00:59:23.902354300Z",
     "start_time": "2023-09-17T00:59:23.889181500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1)  # More numerically stable than softmax\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model instantiation\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights loading\n",
    "\n",
    "Load into the instance of your model the weights you just saved from the adapted notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T00:59:25.278325Z",
     "start_time": "2023-09-17T00:59:23.894646100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "MyModel(\n  (fc): Linear(in_features=784, out_features=10, bias=True)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = model.load_state_dict(th.load(\"./models/rotation_invariant_slfcn.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "\n",
    "To test for a given transformation invariance, you should have pairs of (test) data obtained from the same image: one original, and one transformed.\n",
    "\n",
    "**Hint**: if you want to offload the task to already implemented `torchvision.transforms`, notice (to your advantage) that -- since we are just testing the model -- the dataset needs not to be in shuffled order!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T00:59:25.278325Z",
     "start_time": "2023-09-17T00:59:25.268282200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T00:59:25.504866400Z",
     "start_time": "2023-09-17T00:59:25.273318700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining transforms\n",
    "augmentation = transforms.RandomAffine(degrees=(0, 180), translate=None, scale=None)\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalization = transforms.Normalize(mean=0.1307, std=0.3081)\n",
    "\n",
    "# Defining testing data-sets/loaders\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    # Original: no augmentation\n",
    "    transform=transforms.Compose([to_tensor, normalization]),\n",
    "    download=True,\n",
    ")\n",
    "test_dataset_rot = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    # Transformed: augmented\n",
    "    transform=transforms.Compose([augmentation, to_tensor, normalization]),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader_rot = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation extraction\n",
    "\n",
    "Write a function that extracts the activations of a given layer of the model, in response to a given input. Try to remain as generic as possible, since you may need to re-use it in the future.\n",
    "\n",
    "**Hint**: Look up in the documentation the purpose and features of `hook`s. If you are in trouble, just ask!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T00:59:25.525667900Z",
     "start_time": "2023-09-17T00:59:25.507949700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definition of a hook that outputs layer-specific activations\n",
    "def get_activations(_x, _model, _name):\n",
    "    activations = {}\n",
    "\n",
    "    def get_activation_hook(name):\n",
    "        def hook(_model, _input, _output):\n",
    "            _ = _model, _input\n",
    "            activations[name] = _output.detach()\n",
    "\n",
    "        return hook\n",
    "\n",
    "    layer = getattr(_model, _name)\n",
    "    layer.register_forward_hook(get_activation_hook(\"name\"))\n",
    "    _ = _model(_x)\n",
    "    activation = activations[\"name\"]\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invariance evaluation\n",
    "\n",
    "Recall the definition of *invariance* of (the result of) function $f$ with respect to transformation $g(\\cdot\\;; \\alpha)$ parametrized by $\\alpha$:\n",
    "\n",
    "$$f(g(x; \\alpha))=f(x)\\;\\;\\;\\; \\forall\\alpha$$\n",
    "\n",
    "With the function and data just defined, compare the activations of the model on the original and transformed versions of the same image. Comment on the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T00:59:28.136980100Z",
     "start_time": "2023-09-17T00:59:25.515423200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Euclidean norm of the difference: 11.152886390686035\n",
      "Average Euclidean norm of the difference, control: 12.165810585021973\n"
     ]
    }
   ],
   "source": [
    "differences = []\n",
    "differences_control = []\n",
    "\n",
    "control_img = None\n",
    "\n",
    "for i, (images, label) in enumerate(test_loader):\n",
    "    # The order is the same due to the fact that both dataloaders are not shuffled!\n",
    "    (images_rot, label_rot) = test_loader_rot.__iter__().__next__()\n",
    "\n",
    "    if i == 0:\n",
    "        control_img = th.randn_like(images)\n",
    "\n",
    "    act = get_activations(images, model, \"fc\")\n",
    "    act_rot = get_activations(images_rot, model, \"fc\")\n",
    "    act_control = get_activations(control_img, model, \"fc\")\n",
    "\n",
    "    # When the non-augmented dataset is over, the rest of the batch is eventually filled with augmentations\n",
    "    # This check (and fix) avoids dimension mismatches\n",
    "    if act.shape[0] != act_rot.shape[0]:\n",
    "        act_rot = act_rot[: act.shape[0]]\n",
    "        act_control = act_control[: act.shape[0]]\n",
    "\n",
    "    differences.append(act - act_rot)\n",
    "    differences_control.append(act - act_control)\n",
    "\n",
    "    control_img = images_rot\n",
    "\n",
    "avg_norm = th.linalg.norm(th.cat(differences, dim=0), dim=1).mean()\n",
    "avg_norm_control = th.linalg.norm(th.cat(differences_control, dim=0), dim=1).mean()\n",
    "\n",
    "print(f\"Average Euclidean norm of the difference: {avg_norm.item()}\")\n",
    "print(f\"Average Euclidean norm of the difference, control: {avg_norm_control.item()}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkibvpTEMRILBn2/x8IuJj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
